{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Riniperencsik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "np.random.seed(1)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import string                              # for string operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/Riniperencsik/Desktop/Projects/Twitter Sentiment Analysis/twitter_airlines_data.csv\", encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the tweets based on their sentiment\n",
    "data = data[['airline_sentiment', \"text\"]]\n",
    "positive_tweets = list(data[data['airline_sentiment']=='positive']['text'])\n",
    "negative_tweets = list(data[data['airline_sentiment']=='negative']['text'])\n",
    "neutral_tweets = list(data[data['airline_sentiment']=='neutral']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test sets and training sets\n",
    "# we want to keep the distribution of classes the same for both sets\n",
    "\n",
    "neutral_train = neutral_tweets[:2480] # 2480 is 80% of the neutral tweets\n",
    "\n",
    "negative_train = negative_tweets[:7343] # 7343 is 80% of the negative tweets\n",
    "\n",
    "positive_train = positive_tweets[:1891] # 1891 is 80% of the positive tweets\n",
    "\n",
    "train = positive_train + negative_train + neutral_train\n",
    "\n",
    "\n",
    "labels_train = [] # create the labels for each tweet\n",
    "\n",
    "for i in range(2480): # 0 for neutral\n",
    "    labels_train.append(0)\n",
    "    \n",
    "for i in range(7343): # 1 for negative\n",
    "    labels_train.append(1)\n",
    "    \n",
    "for i in range(1891): # 2 for positive\n",
    "    labels_train.append(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_test = neutral_tweets[2480:] # 20% of the neutral tweets\n",
    "\n",
    "negative_test = negative_tweets[7343:] # 20% of the negative tweets\n",
    "\n",
    "positive_test = positive_tweets[1891:] # 20% of the positive tweets\n",
    "\n",
    "test = positive_test + negative_test + neutral_test\n",
    "\n",
    "\n",
    "labels_test = [] # create the labels for each tweet\n",
    "\n",
    "for i in range(2480, 3099): # 0 for neutral\n",
    "    labels_test.append(0)\n",
    "    \n",
    "for i in range(7343, 9178): # 1 for negative\n",
    "    labels_test.append(1)\n",
    "    \n",
    "for i in range(1891, 2363): # 2 for positive\n",
    "    labels_test.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary_text(tweet0):\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet1= re.sub(r'^RT[\\s]+', '', tweet0)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet1 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet1)\n",
    "\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet1 = re.sub(r'#', '', tweet1)\n",
    "    return tweet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet0):\n",
    "    # instantiate tokenizer class, make all characters the same case and remove all twitter mentions (@)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "    # tokenize tweets\n",
    "    tweet_tokens = tokenizer.tokenize(tweet0)\n",
    "    \n",
    "    return tweet_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_punc(tweet_tokens0):\n",
    "    clean_tweet = [] #create new list to store clean tweet\n",
    "    for word in tweet_tokens0:\n",
    "        if word not in stop_words and word not in string.punctuation: #do not append stopwords or punctuation\n",
    "            clean_tweet.append(word)\n",
    "    return clean_tweet\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem(clean_tweet0):\n",
    "    #create clean list\n",
    "    stem_tweet = []\n",
    "    \n",
    "    #stem each word in the list\n",
    "    for word in clean_tweet0:\n",
    "        stemmed = stemmer.stem(word)\n",
    "        stem_tweet.append(stemmed)\n",
    "    return stem_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_preprocess(tweet0):\n",
    "    tweet0 = remove_unnecessary_text(tweet0)\n",
    "    tweet0 = tokenize(tweet0)\n",
    "    tweet0 = remove_stopwords_punc(tweet0)\n",
    "    tweet0 = stem(tweet0)\n",
    "    return tweet0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Output:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    '''\n",
    "    n = 0  # freqs.get((word, label), 0)\n",
    "\n",
    "    pair = (word, label)\n",
    "    if (pair in freqs):\n",
    "        n = freqs[pair]\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(2363):\n",
    "    labels.append(0)\n",
    "    \n",
    "for i in range(9178):\n",
    "    labels.append(1)\n",
    "    \n",
    "for i in range(3099):\n",
    "    labels.append(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function below preprocesses the tweets then creates a frequency count of each word for each class \n",
    "# results are stored in a dictionary\n",
    "\n",
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    \n",
    "    result = {}\n",
    "    for ys, tweet in zip(ys, tweets):\n",
    "        for word in tweet_preprocess(tweet):\n",
    "            pair = (word, ys)\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "                \n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build frequency count for train and test\n",
    "freqs_train = count_tweets({}, train, labels_train)\n",
    "\n",
    "freqs_test = count_tweets({}, test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fucntion below calculates the logprior (log(number of positive class/ number of negative class))\n",
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood_pos = {}\n",
    "    loglikelihood_neg = {}\n",
    "    loglikelihood_neu = {}\n",
    "    logprior = 0\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = V_pos = V_neg = N_neu = V_neu = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] == 0:\n",
    "            # increment the count of unique neutral words by 1\n",
    "            V_neu += 1 # how many uinque words occur\n",
    "\n",
    "            # Increment the number of neutral words by the count for this (word, label) pair\n",
    "            N_neu += freqs[pair] # how many times each unique word occurs\n",
    "\n",
    "        elif pair[1]==1:\n",
    "            # increment the count of unique negative words by 1\n",
    "            V_neg += 1\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "            \n",
    "        else:\n",
    "            # increment the count of unique positive words by the county by 1\n",
    "            V_pos += 1\n",
    "            \n",
    "            # increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += 1\n",
    "\n",
    "    # Calculate D, the number of tweets\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of neutral tweets\n",
    "    D_pos = (len(list(filter(lambda x: x == 0, train_y))))\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = (len(list(filter(lambda x: x == 1, train_y))))\n",
    "    \n",
    "    # Calculate D_neu, the number of neutral documents\n",
    "    D_neu = (len(list(filter(lambda x: x == 2, train_y))))\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior_pos = D_pos/D\n",
    "    logprior_neg = D_neg/D\n",
    "    logprior_neu = D_neu/D\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs,word,2)\n",
    "        freq_neg = lookup(freqs,word,1)\n",
    "        freq_neu = lookup(freqs,word,0)\n",
    "\n",
    "        # calculate the probability that each word is positive, negative, and neutral\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "        p_w_neu = (freq_neu + 1) / (N_neu + V)\n",
    "\n",
    "        # calculate the log likelihood that the word is pos, neg, and neutral\n",
    "        loglikelihood_pos[word] = (p_w_pos/(p_w_neg+p_w_neu))\n",
    "        loglikelihood_neg[word] = (p_w_neg/(p_w_pos+p_w_neu))\n",
    "        loglikelihood_neu[word] = (p_w_neu/(p_w_neg+p_w_pos))\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return logprior_pos, logprior_neg, logprior_neu, loglikelihood_pos, loglikelihood_neg,  loglikelihood_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability that each word is positive, negative, and neutral for each tweet\n",
    "# for log prior, pos = (number of positive tweets) / all tweets\n",
    "# calculate three different log liklihoods: one for positive, negative, and neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior_pos, logprior_neu, logrprior_neg,\n",
    "                        loglikelihood_pos, loglikelihood_neg, loglikelihood_neu):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = tweet_preprocess(tweet)\n",
    "    \n",
    "    # initialize probability to zero\n",
    "    pos = 0\n",
    "    neg = 0 \n",
    "    neu = 0\n",
    "\n",
    "    # add the logprior\n",
    "    pos += logprior_pos\n",
    "    neg += logprior_neg\n",
    "    neu += logprior_neu\n",
    "    \n",
    "    outcome = []\n",
    "\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood_pos:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            pos += loglikelihood_pos[word]\n",
    "            outcome.append(pos)\n",
    "            \n",
    "        if word in loglikelihood_neg:\n",
    "\n",
    "            neg += loglikelihood_neg[word]\n",
    "            outcome.append(neg)\n",
    "            \n",
    "        if word in loglikelihood_neu:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            neu += loglikelihood_neu[word]\n",
    "            outcome.append(neu)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    if outcome != []:\n",
    "        outcome = max(outcome)\n",
    "    else: \n",
    "        outcome = -1\n",
    "    if outcome == pos:\n",
    "        prediction = 2\n",
    "    elif outcome == neg:\n",
    "        prediction = 1\n",
    "    else: \n",
    "        prediction = 0\n",
    "    print(prediction)\n",
    "    \n",
    "   \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior_pos, logprior_neu, logrprior_neg,\n",
    "                        loglikelihood_pos, loglikelihood_neg, logliklihood_neu):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    y_hats = []\n",
    "    for tweet11 in test_x:\n",
    "        # if the prediction is > 0\n",
    "        prediction =  naive_bayes_predict(tweet11, logprior_pos, logprior_neu, logrprior_neg,\n",
    "                        loglikelihood_pos, loglikelihood_neg, logliklihood_neu) \n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(prediction)\n",
    "    n = 0\n",
    "    num_correct = 0\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    for i in y_hats:\n",
    "        actual = test_y[n]\n",
    "        if i == actual:\n",
    "            num_correct += 1\n",
    "        n += 1\n",
    "    accuracy = num_correct / len(y_hats)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior_pos, logprior_neg, logprior_neu, loglikelihood_pos, loglikelihood_neg, logliklihood_neu = train_naive_bayes(freqs_train, train, labels_train)\n",
    "\n",
    "accuracy = test_naive_bayes(train, labels_train, logprior_pos, logprior_neu, logprior_neg,\n",
    "                        loglikelihood_pos, loglikelihood_neg, logliklihood_neu)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior_pos, logprior_neg, logprior_neu, loglikelihood_pos, loglikelihood_neg, logliklihood_neu = train_naive_bayes(freqs_test, test, labels_train)\n",
    "\n",
    "accuracy = test_naive_bayes(test, labels_test, logprior_pos, logprior_neu, logprior_neg,\n",
    "                        #loglikelihood_pos, loglikelihood_neg, logliklihood_neu)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_naive_bayes([\"i was cold\"], [1], logprior_pos, logprior_neu, logprior_neg,\n",
    "                        loglikelihood_pos, loglikelihood_neg, logliklihood_neu)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
